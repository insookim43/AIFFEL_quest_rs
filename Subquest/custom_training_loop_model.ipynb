{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab08d610",
   "metadata": {},
   "source": [
    "# 사용자 정의 훈련, 평가 루프"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba52781",
   "metadata": {},
   "source": [
    "## 사용자 정의 훈련, 평가 루프가 필요한 경우\n",
    "케라스의 fit() 워크플로우를 사용하는 경우, 지도 학습을 쉽게 진행할 수 있다.  \n",
    "하지만 모든 형태의 머신 러닝을 fit()만으로 수행할 수는 없다. 대표적으로 생성 학습, 자기지도 학습 등.  \n",
    "- 생성 학습  \n",
    "- 자기지도 학습: 타깃을 입력에서 얻음  \n",
    "- 강화 학습 : 강아지를 훈련하는 것처럼 간헐적인 \"보상\"으로 학습됨  \n",
    "\n",
    "\n",
    "이들 모두 명시적으로 타겟을 가지고 있지 않으며,  \n",
    "- 일반적인 지도 학습을 하면서 저수준의 유연성이 필요한 새로운 기능을 추가하고 싶은 경우  \n",
    "\n",
    "에도 사용자 정의 훈련, 평가 로직을 정의해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9542504f",
   "metadata": {},
   "source": [
    "## 전형적인 훈련 루프\n",
    "\n",
    "전형적인 훈련 루프를 구현하기 전, 훈련 루프가 무슨 일을 하는지 알아본다. 다음 과정이 배치 샘플에 대해서 진행된다.\n",
    "\n",
    "1) 현재 배치 데이터에 대한 손실 값을 얻기 위해 `그레이디언트 테이프` 안에서 정방향 패스를 실행함(모델의 출력을 계산함)  \n",
    "2) 모델 가중치에 대한 손실의 그레이디언트를 계산함  \n",
    "3) 현재 배치 데이터에 대한 손실 값을 낮추는 방향으로 모델 가중치를 업데이트함  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8742fcb",
   "metadata": {},
   "source": [
    "### 일반론\n",
    "```predictions = model(inputs)``` 를 통해 단계 1(정방향 계산)을 수행  \n",
    "```gradients = tape_gradient(loss, model.weights)``` 를 통해 단계 2 (그레이디언트 테이프로 계산한 그레이디언트를 추출) 을 수행하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc688ad",
   "metadata": {},
   "source": [
    "### 세부 주의 사항 1 : 훈련과 추론 단계에서 다르게 작동하는 층은 다르게 구현해야 한다.\n",
    "```Dropout``` 층과 같은 일부 케라스 층은 훈련과 추론 시에 동작이 다르다.  \n",
    "이런 층은 call()메서드에 training-> boolean 매개변수를 제공한다.   \n",
    "```dropout(inputs, training=True)```와 같이 호출하면 이전 층의 활성화 출력 값을 일부 랜덤하게 제외한다.  \n",
    "하지만  \n",
    "```dropout(inputs, training=False)```와 같이 호출하면 아무런 일도 수행하지 않는다.  \n",
    "\n",
    "함수형 모델과 Sequential 모델도 call() 메서드에서 training 매개변수를 제공하므로,   \n",
    "정방향 패스에서 케라스 모델을 호출할 때는 training=True로 지정하는 것을 잊지 않도록 하자.  \n",
    "따라서 정방향 패스는 ```predictions = model(inputs, training=True)``` 가 된다.  \n",
    "\n",
    "(*주: 서브클래싱의 경우는 직접 call()메서드에 training 매개변수를 정의하고   \n",
    "훈련, 추론에 따라 동작이 달라지는 층들을 개별적으로 제어해 주어야 한다.)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9711d37a",
   "metadata": {},
   "source": [
    "### 세부 주의 사항 2 : 모델 가중치 그레이디언트를 추출할 때 훈련가능한 가중치만을 호출한다.\n",
    "\n",
    "```tape.gradients(loss, model.weights) 가 아니라 tape.gradients(loss, model.trainable_weights)```를 사용해야 한다.  \n",
    "층과 모델에는 두 종류의 가중치가 있다.  \n",
    "\n",
    "- 훈련 가능한 가중치: Dense 층의 커널과 편향처럼 모델의 손실을 최소화하기 위해 역전파로 업데이트됨\n",
    "- 훈련되지 않는 가중치: 해당 층의 정방향 패스 동안 업데이트됨. 예를 들어 얼마나 많은 배치를 처리했는지 카운트하는 사용자 정의 층이 필요하다면 이 정보를 훈련되지 않는 가중치에 저장하고 배치마다 값을 1씩 증가시킴.  \n",
    "\n",
    "케라스에 내장된 층 중에 훈련되지 않는 가중치를 가진 층은 Batch Normalization 뿐이다.  \n",
    "BatchNormalization 층은 데이터의 평균과 표준 편차에 대한 정보를 추적하여,   \n",
    "특성 정규화(feature normalization)를 실시간으로 근사하기 위해 훈련되지 않는 가중치가 필요하다.\n",
    "\n",
    "이 두 가지를 고려하여 지도 학습을 위한 훈련 스텝을 다음과 같이 작성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be0ced8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60149023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape: \n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(targets, predictions)\n",
    "    gradients = tape.gradients(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(model.trainable_weights, gradients))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeeda0d",
   "metadata": {},
   "source": [
    "## 훈련 루프에서 측정 지표의 저수준 사용법\n",
    "\n",
    "저수준 훈련 루프에서도 케라스 지표(사용자 정의 지표, 내장 지표를 막론하고) 를 사용하게 된다.\n",
    "측정 지표 API는 각 배치의 타깃과 예측에 대해 update_state(y_true, y_pred) 를 호출하면 된다.  \n",
    "그리고 result() 메서드를 사용하여 현재 지표 값을 얻는다.  \n",
    "훈련 에포크나 평가를 시작할 때처럼 현재 결과를 재설정하기 위해서는 metric.reset_state()를 사용하는 것을 주의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e0af585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과: 1.00\n"
     ]
    }
   ],
   "source": [
    "metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "targets = [0,1,2]\n",
    "predictions = [[1,0,0], [0,1,0], [0,0,1]]\n",
    "metric.update_state(targets, predictions)\n",
    "current_result = metric.result()\n",
    "print(f\"결과: {current_result:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e400eb",
   "metadata": {},
   "source": [
    "## 완전한 훈련과 평가 루프\n",
    "\n",
    "정방향 패스, 역방향 패스, 지표 추적을 fit()과 유사한 훈련 스텝 함수로 연결해 보자.  \n",
    "이 함수는 데이터와 타깃의 배치를 받고 fit() 진행 표시줄이 출력하는 로그를 반환한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "039bc1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_mnist_model():\n",
    "    inputs = keras.Input(shape=(28*28,))\n",
    "    features = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "    features = layers.Dropout(0.5)(features)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b5a8a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_mnist_model()\n",
    "\n",
    "#손실함수 정의\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "#옵티마이저 정의\n",
    "optimizer = keras.optimizers.RMSprop()\n",
    "#모니터링할 지표 리스트 준비\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    "#손실 평균을 추적할 평균 지표 준비\n",
    "loss_tracking_metric = keras.metrics.Mean()\n",
    "\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(targets, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    logs = {} \n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs[metric.name] = metric.result()\n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs[\"loss\"] = loss_tracking_metric.result()\n",
    "    return logs\n",
    "\n",
    "# 매 에폭 시작과 평가 전에 지표의 상태를 재설정하는 유틸리티 함수\n",
    "def reset_metrics():\n",
    "    for metric in metrics:\n",
    "        metric.reset_state()\n",
    "    loss_tracking_metric.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4237450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완전한 훈련 루프\n",
    "\n",
    "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
    "images = images.reshape((60000, 28*28)).astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28*28)).astype(\"float32\") / 255\n",
    "train_images, val_images = images[10000:], images[:10000]\n",
    "train_labels, val_labels = labels[10000:], labels[:10000]\n",
    "# tf.data.Dataset 객체를 사용하여 넘파이 데이터를 크기가 32인 배치로 데이터를 순회하는 iterator로 바꾼다.\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "(train_images, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41db66b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 에포크 결과\n",
      "...sparse_categorical_accuracy: 0.9140\n",
      "...loss: 0.2921\n",
      "1 번째 에포크 결과\n",
      "...sparse_categorical_accuracy: 0.9528\n",
      "...loss: 0.1685\n",
      "2 번째 에포크 결과\n",
      "...sparse_categorical_accuracy: 0.9614\n",
      "...loss: 0.1407\n"
     ]
    }
   ],
   "source": [
    "training_dataset = training_dataset.batch(32)\n",
    "epochs=3\n",
    "for epoch in range(epochs):\n",
    "    reset_metrics()\n",
    "    for inputs_batch, targets_batch in training_dataset:\n",
    "        logs = train_step(inputs_batch, targets_batch)\n",
    "    print(f\"{epoch} 번째 에포크 결과\")\n",
    "    for key, value in logs.items():\n",
    "        print(f\"...{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9bb6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 루프 : test_step() 은 train_step()함수에서 모델의 가중치를 업데이트하는 코드가 빠져 있음(GradientTape와 옵티마이저에 관련된 부분)\n",
    "def test_step(inputs, targets):\n",
    "    predictions = model(inputs, training=False)\n",
    "    loss = loss_fn(targets, predictions)\n",
    "    \n",
    "    logs = {}\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs[\"val_\" + metric.name] = metric.result()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
